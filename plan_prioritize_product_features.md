## Data collection
### Objectives / metrics
* User engagement (0 - 10 score): impact on optimization run frequency across users; start with estimates, replace with telemetry.
* Development effort (engineer‑weeks): total engineering time to implement; estimate from past velocity, refine with actual burndown.
* Market uniqueness (0 - 100 %): % of differentiating features present vs top 3 alternatives and relative pricing premium/discount; driven by competitive scan.
* Decision clarity (0 - 10 score): user‑reported confidence + success rate and task‑completion time in moderated UX tests and in‑product analytics.
* Scalability (max concurrent runs): load‑test estimates first, then production metrics as usage grows
* Operating cost ($/run): Per run cost marginal impacts
* Data capture (fields): number of fields stored per optimization problem and results, also look at auto/AI vs manual

### Data sources
* App telemetry / logs
* Development tooling reports & past project velocity
* Competitive analysis (feature & pricing comparisons)
* Moderated UX‑lab tests and in‑app user feedback
* Load‑test results (concurrency & latency)
* Cloud pricing calculators, LLM token‑usage logs
* Public SaaS engagement benchmarks (Heap, Mixpanel, etc.)
* Data‑quality monitoring dashboards

## Results

### Analysis

### Decision