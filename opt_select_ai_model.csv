option_name,response_time,reasoning,blended_cost,coherence,api_utility,context_window,implementation_complexity,notes
o3,9.0,92,25.00,8.0,9,128000,1,
Claude 3.7 Sonnet,0.92,92,9.00,9.0,9,200000,1,
Qwen3-235B-A22B (Reasoning),8.5,92,0.20,9.0,6,128000,2,
Gemini 2.5 Pro Preview,12.0,91,5.625,9.0,8,2000000,1,
o4-mini (high),42.0,88,2.75,9.0,9,128000,1,
o3-mini,13.5,85,2.75,8.5,9,200000,1,"High Confidence Revision: context_window 128k -> 200k"
Gemini 2.5 Flash Preview,2.5,84,0.375,8.0,8,1000000,1,
Cohere Command R+,11.0,80,6.25,8.0,8,128000,1,"High Confidence Revision: blended_cost 1.00 -> 6.25 (based on Aug '24 pricing)"
Claude 3.5 Haiku,3.5,80,2.40,8.0,9,200000,1,"High Confidence Revision: blended_cost 0.75 -> 2.40 (original CSV had cost for older Claude 3 Haiku)"
DeepSeek R1,4.5,78,0.18,8.0,6,128000,1,
Mistral Large 2,25.0,77,4.00,8.0,7,128000,2,
Grok-3,9.9,76,9.00,8.0,6,128000,1,
Mixtral 8Ã—22B Instruct,8.0,75,4.00,7.5,6,64000,2,
Cohere Command R,4.0,75,0.15,7.5,8,128000,1,
Llama 4 Maverick 17B,4.0,74,0.525,7.0,6,1000000,2,"High Confidence Revision: blended_cost 0.30 -> 0.525 (based on Artificial Analysis API pricing)"
GPT-4.1 mini,5.6,73,2.00,8.0,9,128000,1,
Llama 4 Scout,4.6,72,0.35,7.0,6,10000000,2,"High Confidence Revision: blended_cost 0.25 -> 0.35 (based on Artificial Analysis API pricing)"
Grok 3 mini,5.0,72,0.40,7.5,6,128000,1,
Gemini 2.0 Flash,2.4,70,0.25,7.5,8,1000000,1,
Claude 3.5 Sonnet (Oct),7.1,70,9.00,8.0,8,200000,1,
GPT-4 Turbo,6.0,65,20.00,9.5,10,128000,1,
GPT-4o (Nov '24),0.50,60,12.50,9.5,10,128000,1,
Gemma 3 27B,4.0,55,0.10,6.5,5,128000,2,
Claude 3 Haiku,0.8,50,0.75,6.0,5,200000,1,